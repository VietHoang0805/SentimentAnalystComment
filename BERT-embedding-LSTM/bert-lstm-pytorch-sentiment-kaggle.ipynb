{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11498854,"sourceType":"datasetVersion","datasetId":7208635}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %pip install torch\n# %pip install transformers\n# %pip install pandas\n# %pip install numpy\n# %pip install scikit-learn\n# %pip install gradio\n# %pip install safetensors","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom tqdm import tqdm\nimport os\nfrom safetensors.torch import save_file","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SEQ_LEN = 256\nBATCH_SIZE = 16\nEPOCHS = 10\nLR = 2e-5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sentimentdataset/NTC_SV/NTC_SV_train.csv\").dropna()\ntexts = df['review'].tolist()\nlabels = LabelEncoder().fit_transform(df['label'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FoodyDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.1, random_state=42)\ntrain_dataset = FoodyDataset(X_train, y_train, tokenizer, SEQ_LEN)\nval_dataset = FoodyDataset(X_val, y_val, tokenizer, SEQ_LEN)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/sentimentdataset/NTC_SV/NTC_SV_test.csv\").dropna()\nX_test = df_test['review'].tolist()\ny_test = LabelEncoder().fit(df_test['label']).transform(df_test['label'])\ntest_dataset = FoodyDataset(X_test, y_test, tokenizer, SEQ_LEN)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BERTLSTMClassifier(nn.Module):\n    def __init__(self, hidden_dim=128, num_classes=3):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n        self.lstm = nn.LSTM(768, hidden_dim, batch_first=True, bidirectional=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        lstm_out, _ = self.lstm(outputs.last_hidden_state)\n        pooled = torch.mean(lstm_out, dim=1)\n        out = self.dropout(pooled)\n        return self.fc(out)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = BERTLSTMClassifier(num_classes=len(set(labels))).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, data_loader):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    for batch in tqdm(data_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    acc = accuracy_score(all_labels, all_preds)\n    return total_loss / len(data_loader), acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SAVE_EVERY = 2  # Lưu mô hình mỗi 2 epoch, có thể thay đổi tuỳ ý\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.makedirs(\"checkpoints\", exist_ok=True)\n\nfor epoch in range(EPOCHS):\n    loss, acc = train_epoch(model, train_loader)\n    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {loss:.4f} - Accuracy: {acc:.4f}\")\n    \n    if (epoch + 1) % SAVE_EVERY == 0:\n        save_path = f\"checkpoints/bert_lstm_epoch{epoch+1}.safetensors\"\n        save_file(model.state_dict(), save_path)\n        print(f\"✔️ Saved checkpoint at {save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 11. Đánh giá trên tập test\ndef evaluate(model, data_loader):\n    model.eval()\n    predictions, true_labels = [], []\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].cuda()\n            attention_mask = batch['attention_mask'].cuda()\n            labels = batch['label'].cuda()\n\n            outputs = model(input_ids, attention_mask)\n            preds = torch.argmax(outputs, dim=1)\n\n            predictions.extend(preds.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n\n    print(classification_report(true_labels, predictions))\n\nprint(\"=== Evaluation on Test Set ===\")\nevaluate(model, test_loader)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_file(model.state_dict(), \"bert_lstm_foody.safetensors\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}